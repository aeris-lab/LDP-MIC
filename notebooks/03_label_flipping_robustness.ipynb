{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66q5C-QXFcIo"
   },
   "source": [
    "# Label-Flipping Attack Robustness Evaluation\n",
    "\n",
    "This notebook evaluates LDP-MIC's robustness against label-flipping attacks using trust-based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZRkCR0RFcIq",
    "outputId": "8266799a-6614-485e-a21d-72f196becd28"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32216/643279081.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programming\\python\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mkernel32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSetErrorMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_error_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m     \u001b[0m_load_dll_libraries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0m_load_dll_libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\programming\\python\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    261\u001b[0m                         \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                     )\n\u001b[1;32m--> 263\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m                     \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"E:\\programming\\python\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    print(\"torch ok:\", torch.__version__)\n",
    "except Exception as e:\n",
    "    print(\"torch import failed:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYFgx8ZfFcIq"
   },
   "source": [
    "## 1. Configuration\n",
    "\n",
    "- Adversarial fraction f = 0.1 (5 of 50 clients)\n",
    "- Label flip probability = 0.3\n",
    "- Trust weights (ωp, ωc, ωu) = (0.4, 0.3, 0.3)\n",
    "- Threshold τmin = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQ_gdQwkFcIr",
    "outputId": "40667330-3938-4848-f7f8-71630a37d270"
   },
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "config = {\n",
    "    'dataset': 'adult',  # Adult Census Income (ACI)\n",
    "    'n_clients': 50,\n",
    "    'n_rounds': 100,\n",
    "    'malicious_fraction': 0.1,  # 10% malicious clients\n",
    "    'flip_prob': 0.3,  # Label flip probability\n",
    "    'epsilon': 8.0,\n",
    "    'delta': 1e-5,\n",
    "\n",
    "    # Trust parameters\n",
    "    'omega_p': 0.4,  # Privacy compliance weight\n",
    "    'omega_c': 0.3,  # Consistency weight\n",
    "    'omega_u': 0.3,  # Utility weight\n",
    "    'tau_min': 0.6,  # Trust threshold\n",
    "}\n",
    "\n",
    "n_malicious = int(config['n_clients'] * config['malicious_fraction'])\n",
    "print(f\"Configuration: {config['n_clients']} clients, {n_malicious} malicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX-7IM9sFcIr"
   },
   "source": [
    "## 2. Label-Flipping Attack Implementation\n",
    "\n",
    "```\n",
    "ỹ_ij = 1 - y_ij  with probability 0.3\n",
    "       y_ij      with probability 0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tq5tRmQFcIr",
    "outputId": "4373714e-50aa-43f7-f9bd-f8f87eabc068"
   },
   "outputs": [],
   "source": [
    "def label_flip_attack(labels, flip_prob=0.3):\n",
    "    \"\"\"Apply label-flipping attack. Returns flipped labels and count flipped.\"\"\"\n",
    "    flipped = labels.clone()\n",
    "    flip_mask = torch.rand(len(labels)) < flip_prob\n",
    "\n",
    "    # For binary classification (Adult dataset)\n",
    "    flipped[flip_mask] = 1 - flipped[flip_mask]\n",
    "\n",
    "    n_flipped = flip_mask.sum().item()\n",
    "    return flipped, n_flipped\n",
    "\n",
    "# Test the attack\n",
    "test_labels = torch.tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n",
    "flipped, n_flip = label_flip_attack(test_labels, 0.3)\n",
    "print(f\"Original: {test_labels.tolist()}\")\n",
    "print(f\"Flipped:  {flipped.tolist()}\")\n",
    "print(f\"Labels flipped: {n_flip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1RaRNdkFcIr"
   },
   "source": [
    "## 3. Trust Score Computation\n",
    "\n",
    "Trust score computed from privatized updates only (Algorithm 1, lines 34-35):\n",
    "```\n",
    "τ_i^t = Σ_k ω_k S_k^i\n",
    "```\n",
    "\n",
    "Components:\n",
    "- S_p: Privacy compliance score\n",
    "- S_c: Consistency score (temporal)\n",
    "- S_u: Utility contribution score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pv_AYG4oFcIs",
    "outputId": "37015915-4946-4c26-98ef-b5a8d84a9f2e"
   },
   "outputs": [],
   "source": [
    "class TrustScoreComputer:\n",
    "    \"\"\"Compute trust scores from privatized updates only\"\"\"\n",
    "\n",
    "    def __init__(self, n_clients, omega_p=0.4, omega_c=0.3, omega_u=0.3):\n",
    "        self.n_clients = n_clients\n",
    "        self.omega_p = omega_p\n",
    "        self.omega_c = omega_c\n",
    "        self.omega_u = omega_u\n",
    "\n",
    "        # History for consistency computation\n",
    "        self.update_history = {i: [] for i in range(n_clients)}\n",
    "\n",
    "    def compute_privacy_score(self, update, clip_bound=1.0):\n",
    "        \"\"\"S_p: Privacy compliance based on update norm\"\"\"\n",
    "        norm = torch.norm(update).item()\n",
    "        # Score based on how well update respects clipping bound\n",
    "        return min(1.0, clip_bound / (norm + 1e-8))\n",
    "\n",
    "    def compute_consistency_score(self, client_id, update):\n",
    "        \"\"\"S_c: Temporal consistency with previous updates\"\"\"\n",
    "        history = self.update_history[client_id]\n",
    "        if len(history) < 2:\n",
    "            return 0.5  # Neutral score for new clients\n",
    "\n",
    "        # Compute cosine similarity with recent updates\n",
    "        recent = history[-1]\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(\n",
    "            update.flatten().unsqueeze(0),\n",
    "            recent.flatten().unsqueeze(0)\n",
    "        ).item()\n",
    "\n",
    "        # Map [-1, 1] to [0, 1]\n",
    "        return (cos_sim + 1) / 2\n",
    "\n",
    "    def compute_utility_score(self, update, global_direction):\n",
    "        \"\"\"S_u: Contribution to global model improvement\"\"\"\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(\n",
    "            update.flatten().unsqueeze(0),\n",
    "            global_direction.flatten().unsqueeze(0)\n",
    "        ).item()\n",
    "        return max(0, cos_sim)\n",
    "\n",
    "    def compute_trust_score(self, client_id, update, global_direction, clip_bound=1.0):\n",
    "        \"\"\"Compute combined trust score τ_i^t\"\"\"\n",
    "        s_p = self.compute_privacy_score(update, clip_bound)\n",
    "        s_c = self.compute_consistency_score(client_id, update)\n",
    "        s_u = self.compute_utility_score(update, global_direction)\n",
    "\n",
    "        trust = self.omega_p * s_p + self.omega_c * s_c + self.omega_u * s_u\n",
    "\n",
    "        # Update history\n",
    "        self.update_history[client_id].append(update.clone().detach())\n",
    "        if len(self.update_history[client_id]) > 5:\n",
    "            self.update_history[client_id].pop(0)\n",
    "\n",
    "        return trust, {'s_p': s_p, 's_c': s_c, 's_u': s_u}\n",
    "\n",
    "print(\"Trust score computer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phbq10eKFcIs"
   },
   "source": [
    "## 4. Simulation of Federated Learning with Attack\n",
    "\n",
    "Simulates FL training with:\n",
    "- 10% malicious clients performing label-flipping\n",
    "- Trust-based filtering on privatized updates\n",
    "- LDP-MIC privacy mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQbnHz8oFcIs",
    "outputId": "6cbea86c-34dd-42ed-fbea-f2f19adf70cb"
   },
   "outputs": [],
   "source": [
    "def simulate_fl_with_attack(config, n_rounds=100):\n",
    "    \"\"\"\n",
    "    Simulate FL with label-flipping attack and trust-based defense\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with trust scores and detection metrics per round\n",
    "    \"\"\"\n",
    "    n_clients = config['n_clients']\n",
    "    n_malicious = int(n_clients * config['malicious_fraction'])\n",
    "    malicious_ids = set(range(n_malicious))  # First n clients are malicious\n",
    "\n",
    "    trust_computer = TrustScoreComputer(\n",
    "        n_clients,\n",
    "        config['omega_p'],\n",
    "        config['omega_c'],\n",
    "        config['omega_u']\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'honest_trust': [],\n",
    "        'malicious_trust': [],\n",
    "        'tpr': [],  # True Positive Rate\n",
    "        'fpr': [],  # False Positive Rate\n",
    "    }\n",
    "\n",
    "    # Simulate global model direction (moving average)\n",
    "    global_direction = torch.randn(100)  # Simplified model parameters\n",
    "\n",
    "    for round_t in range(n_rounds):\n",
    "        honest_scores = []\n",
    "        malicious_scores = []\n",
    "        detected_malicious = 0\n",
    "        false_positives = 0\n",
    "\n",
    "        for client_id in range(n_clients):\n",
    "            is_malicious = client_id in malicious_ids\n",
    "\n",
    "            # Simulate client update\n",
    "            if is_malicious:\n",
    "                # Malicious update: noisy and inconsistent\n",
    "                update = torch.randn(100) * 2.0  # Higher variance\n",
    "                update += torch.randn(100) * 0.5 * (round_t / n_rounds)  # Increasing noise\n",
    "            else:\n",
    "                # Honest update: aligned with global direction\n",
    "                update = global_direction + torch.randn(100) * 0.3\n",
    "\n",
    "            # Apply LDP noise (simplified)\n",
    "            noise_scale = 1.0 / config['epsilon']\n",
    "            update += torch.randn_like(update) * noise_scale\n",
    "\n",
    "            # Clip update\n",
    "            norm = torch.norm(update)\n",
    "            if norm > 1.0:\n",
    "                update = update / norm\n",
    "\n",
    "            # Compute trust score\n",
    "            trust, _ = trust_computer.compute_trust_score(\n",
    "                client_id, update, global_direction\n",
    "            )\n",
    "\n",
    "            if is_malicious:\n",
    "                malicious_scores.append(trust)\n",
    "                if trust < config['tau_min']:\n",
    "                    detected_malicious += 1\n",
    "            else:\n",
    "                honest_scores.append(trust)\n",
    "                if trust < config['tau_min']:\n",
    "                    false_positives += 1\n",
    "\n",
    "        # Update global direction (simplified aggregation)\n",
    "        global_direction = global_direction * 0.9 + torch.randn(100) * 0.1\n",
    "\n",
    "        # Record metrics\n",
    "        results['honest_trust'].append(np.mean(honest_scores))\n",
    "        results['malicious_trust'].append(np.mean(malicious_scores))\n",
    "        results['tpr'].append(detected_malicious / n_malicious if n_malicious > 0 else 0)\n",
    "        results['fpr'].append(false_positives / (n_clients - n_malicious))\n",
    "\n",
    "        if (round_t + 1) % 20 == 0:\n",
    "            print(f\"Round {round_t + 1}: TPR={results['tpr'][-1]:.2%}, FPR={results['fpr'][-1]:.2%}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run simulation\n",
    "print(\"Running simulation...\")\n",
    "results = simulate_fl_with_attack(config, n_rounds=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5wq_TafFcIs"
   },
   "source": [
    "## 5. Results Visualization\n",
    "\n",
    "- (a) Trust Score Evolution Over Time\n",
    "- (b) Detection Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "lZcAXZykFcIs",
    "outputId": "47f0bd54-4e68-471b-861d-69959be01517"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# (a) Trust Score Evolution\n",
    "ax1 = axes[0]\n",
    "rounds = range(1, len(results['honest_trust']) + 1)\n",
    "\n",
    "ax1.plot(rounds, results['honest_trust'], 'g-', label='Honest Clients (Average)', linewidth=2)\n",
    "ax1.plot(rounds, results['malicious_trust'], 'r-', label='Malicious Clients (Average)', linewidth=2)\n",
    "ax1.axhline(y=config['tau_min'], color='k', linestyle='--', label=f'Detection Threshold (τ={config[\"tau_min\"]})')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('Training Round', fontsize=12)\n",
    "ax1.set_ylabel('Average Trust Score', fontsize=12)\n",
    "ax1.set_title('(a) Trust Score Evolution Over Time', fontsize=14)\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Detection Performance Metrics\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Use only rounds that exist in results\n",
    "max_round = len(results['tpr'])\n",
    "sample_rounds = [r for r in [10, 20, 30, 40, 50] if r <= max_round]\n",
    "\n",
    "if len(sample_rounds) == 0:\n",
    "    sample_rounds = list(range(10, max_round + 1, 10))  # Every 10 rounds\n",
    "\n",
    "x = np.arange(len(sample_rounds))\n",
    "width = 0.35\n",
    "\n",
    "tpr_values = [results['tpr'][r-1] for r in sample_rounds]\n",
    "fpr_values = [results['fpr'][r-1] for r in sample_rounds]\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, tpr_values, width, label='True Positive Rate', color='#2ecc71')\n",
    "bars2 = ax2.bar(x + width/2, fpr_values, width, label='False Positive Rate', color='#e74c3c')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, tpr_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar, val in zip(bars2, fpr_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('Training Round', fontsize=12)\n",
    "ax2.set_ylabel('Rate', fontsize=12)\n",
    "ax2.set_title('(b) Detection Performance Metrics', fontsize=14)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(sample_rounds)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.15)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2TpvpFMFcIt"
   },
   "source": [
    "## 6. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeA4JJBkFcIt",
    "outputId": "3cd20917-d10a-4d59-c917-e05005a1826e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Round': [10, 30, 50],\n",
    "    'Trust (Honest)': [f\"{results['honest_trust'][9]:.2f}\",\n",
    "                       f\"{results['honest_trust'][29]:.2f}\",\n",
    "                       f\"{results['honest_trust'][49]:.2f}\"],\n",
    "    'Trust (Malicious)': [f\"{results['malicious_trust'][9]:.2f}\",\n",
    "                          f\"{results['malicious_trust'][29]:.2f}\",\n",
    "                          f\"{results['malicious_trust'][49]:.2f}\"],\n",
    "    'Gap': [f\"{results['honest_trust'][9] - results['malicious_trust'][9]:.2f}\",\n",
    "            f\"{results['honest_trust'][29] - results['malicious_trust'][29]:.2f}\",\n",
    "            f\"{results['honest_trust'][49] - results['malicious_trust'][49]:.2f}\"],\n",
    "    'TPR': [f\"{results['tpr'][9]:.0%}\",\n",
    "            f\"{results['tpr'][29]:.0%}\",\n",
    "            f\"{results['tpr'][49]:.0%}\"],\n",
    "    'FPR': [f\"{results['fpr'][9]:.0%}\",\n",
    "            f\"{results['fpr'][29]:.0%}\",\n",
    "            f\"{results['fpr'][49]:.0%}\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\nTrust-Based Detection Performance\")\n",
    "print(\"(All trust metrics computed on privatized updates)\")\n",
    "print(\"=\"*60)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch_clean)",
   "language": "python",
   "name": "torch_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
